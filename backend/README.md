# Byte in Bottle - API

> Powered by bytes. Driven by attitude.

FastAPI backend with Ollama integration for AI-powered chat and text generation.

## Features

- üöÄ **FastAPI** - Modern, fast web framework for building APIs
- ü§ñ **Ollama Integration** - Local LLM support with Ollama
- ‚ö° **UV Package Manager** - Lightning-fast Python package management by Astral
- üîÑ **CORS Enabled** - Ready for frontend integration
- üìù **Auto-generated Docs** - Interactive API documentation with Swagger UI

## Prerequisites

### For Docker (Recommended)

- [Docker](https://www.docker.com/get-started)
- [Docker Compose](https://docs.docker.com/compose/install/)

### For Local Development

- Python 3.11+
- [uv](https://github.com/astral-sh/uv) - Install with: `curl -LsSf https://astral.sh/uv/install.sh | sh`
- [Ollama](https://ollama.ai/) - Install and have it running locally

## Quick Start

### Using Docker Compose (Recommended)

From the project root directory:

```bash
# Start both api and Ollama services
docker-compose up -d

# Stop services
docker-compose down

# Rebuild after code changes
docker-compose up -d --build
```

The services will be available at:

- Backend API: <http://localhost:8000>
- Interactive docs: <http://localhost:8000/docs>
- Ollama: <http://localhost:11434>

### Local Development Setup

1. **Install dependencies:**

   ```bash
   uv sync
   ```

2. **Set up environment variables:**

   ```bash
   cp .env.example .env
   # Edit .env as needed
   ```

3. **Ensure Ollama is running:**

   ```bash
   # Pull a model (if you haven't already)
   ollama pull llama3.2
   ```

4. **Run the server:**

   ```bash
   # Using uv
   uv run api
   
   # Or directly with uvicorn
   uv run uvicorn api.main:app --reload
   ```

5. **Access the API:**
   - API: <http://localhost:8000>
   - Interactive docs: <http://localhost:8000/docs>
   - ReDoc: <http://localhost:8000/redoc>

## API Endpoints

### Health Check

```bash
GET /health
```

### List Models

```bash
GET /models
```

### Chat Completion

```bash
POST /chat
Content-Type: application/json

{
  "model": "llama3.2",
  "messages": [
    {"role": "user", "content": "Hello!"}
  ]
}
```

### Text Generation

```bash
POST /generate?model=llama3.2&prompt=Hello!
```

## Development

```bash
# Run with auto-reload
uv run uvicorn api.main:app --reload --host 0.0.0.0 --port 8000

# Add new dependencies
uv add <package-name>

# Update dependencies
uv sync
```

## Project Structure

```raw
backend/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ api/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ main.py          # FastAPI application
‚îú‚îÄ‚îÄ .env.example             # Environment variables template
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ pyproject.toml           # Project configuration
‚îî‚îÄ‚îÄ README.md
```

## Environment Variables

- `HOST` - Server host (default: 0.0.0.0)
- `PORT` - Server port (default: 8000)
- `ALLOWED_ORIGINS` - CORS allowed origins (comma-separated)
- `OLLAMA_HOST` - Ollama server URL
  - Local development: `http://localhost:11434`
  - Docker Compose: `http://ollama:11434` (automatically configured)

## Docker

### Building the Image

```bash
# From the api directory
docker build -t byte-in-bottle-api .

# Or from project root
docker build -t byte-in-bottle-api ./api
```

### Running with Docker

```bash
# Run the api container (requires Ollama running separately)
docker run -d \
  -p 8000:8000 \
  -e OLLAMA_HOST=http://host.docker.internal:11434 \
  --name byte-in-bottle-api \
  byte-in-bottle-api
```

### Docker Compose

The recommended way is to use the `docker-compose.yml` file in the project root, which orchestrates both the api and Ollama services.

## License

See LICENSE in the root directory.
